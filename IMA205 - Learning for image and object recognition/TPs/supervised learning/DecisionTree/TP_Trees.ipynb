{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8OIJnHSo1_T"
      },
      "source": [
        "# Alzheimer prediction using gray matter density from T1w MRI "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN4bwTpJo1_Z"
      },
      "source": [
        "**Deadline**: Upload this notebook (rename it as 'TP4-Trees-YOUR-SURNAME.ipynb') with your answers and code to the Moodle/Ecampus before the deadline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1qko7fo1_g"
      },
      "source": [
        "We will use a dataset composed of neuroimaging features from brain T1w MR images of 752 subjects, 416 controls and 336 with Alzheimer’s disease. Following the pipeline described in [1], all images are first normalized to a\n",
        "common space, providing a voxel-wise correspondence across subjects. Then, gray matter density is computed at each voxel and averaged over a set of ROIs (Region of Interest) of an atlas, at the beginning you will use the [AAL2 atlas](http://www.gin.cnrs.fr/en/tools/aal/). Data comes from several freely available datasets, like [ADNI](http://adni.loni.usc.edu/) and [OASIS](https://www.oasis-brains.org/), and has been pre-processed by the [Clinica](http://www.clinica.run/) team using the procedure explained in [1].\n",
        "\n",
        "Please load the data from the file: *dataTP.npz* where *T1xxxx* is a matrix containing the averaged density (each row is a subject and each column a feature), *y* is a vector containing the diagnosis (0 for controls and 1 for Alzheimer’s patients) and *ROIlabelsx* contains the name of the ROI of each feature. Here, *x* can take the name of the three atlases you have at your disposal: AAL2, [AICHA](http://www.gin.cnrs.fr/fr/outils/aicha/), [HAMMERS](https://brain-development.org/brain-atlases/adult-brain-atlases/).\n",
        "\n",
        "**Reference**:\n",
        "[1] J. Samper-González, N. Burgos, S. Bottani, S. Fontanella, P. Lu, A. Marcoux, A. Routier, J. Guillon, M. Bacci, J. Wen, A. Bertrand, H. Bertin, M.-O. Habert, S. Durrleman, T. Evgeniou, O. Colliot. *Reproducible evaluation of classification methods in Alzheimer's disease: framework and application to MRI and PET data*. NeuroImage, 2018 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdBPjnmyo1_k"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=ImportWarning)\n",
        "\n",
        "# Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install -q nilearn\n",
        "from nilearn import plotting\n",
        "%matplotlib inline\n",
        "np.random.seed(seed=666)\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TK-_zzzFUyE"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='11cQmPm64k3T7ml5fPLetZgb1j1AjHBH8',\n",
        "dest_path='./dataTP.npz')\n",
        "gdd.download_file_from_google_drive(file_id='1S7e5IrPygE4VV0JTwqJIlyO2S_NhsiI4',\n",
        "dest_path='./AtlasAAL2.nii')\n",
        "gdd.download_file_from_google_drive(file_id='1E0pu5jIMpgcs2DQ8lBGWliwEBZvKrnV9',\n",
        "dest_path='./AtlasAICHA.nii')\n",
        "gdd.download_file_from_google_drive(file_id='1yltKwULrkHYh79RAh_zAg08r8pQMjRlQ',\n",
        "dest_path='./AtlasHAMMERS.nii')\n",
        "\n",
        "with np.load('./dataTP.npz',allow_pickle=True) as data:\n",
        "    T1AAL2 = data['T1AAL2'] # data from AAL2 Atlas\n",
        "    T1AICHA = data['T1AICHA'] # data from AICHA Atlas\n",
        "    T1HAMMERS = data['T1HAMMERS'] # data from HAMMERS Atlas  \n",
        "    y = data['y'] # classes, 0 for controls and 1 for patients    \n",
        "    ROIlabelsAAL2 = data['ROIlabelsAAL2'] # labels for ROIs of atlas AAL2 \n",
        "    ROIlabelsAICHA = data['ROIlabelsAICHA']    # labels for ROIs of atlas AICHA \n",
        "    ROIlabelsHAMMERS = data['ROIlabelsHAMMERS']    # labels for ROIs of atlas HAMMERS "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kuwBwFIilJ"
      },
      "source": [
        "# Choose Atlas (here AAL2)\n",
        "X=T1AAL2 # T1AAL2, T1AICHA, T1HAMMERS\n",
        "labels=ROIlabelsAAL2 # ROIlabelsAAL2, ROIlabelsAICHA, ROIlabelsHAMMERS\n",
        "atlas='./AtlasAAL2.nii' #AtlasAAL2.nii, AtlasAICHA.nii, AtlasHAMMERS.nii\n",
        "\n",
        "N,M = X.shape # number subjects and ROIs\n",
        "class_names = [\"control\",\"alzheimer\"] # y=0, y=1\n",
        "\n",
        "print('Number of controls and Alzheimer patients is respectively: {0} and {1}'.format(N-np.sum(y), np.sum(y)))\n",
        "print('Number of ROI (features) is: {0}'.format(M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pk4GnWxo1_7"
      },
      "source": [
        "Using the library nilearn we can also plot the atlas used to define the ROIs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LUtw8L2o1_-"
      },
      "source": [
        "plotting.plot_roi(atlas, title=atlas)\n",
        "plotting.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FIMJctdo2AE"
      },
      "source": [
        "In this TP we will use Decision Trees, Bagging and Random Forests. Let's start with Decision Trees. First of all, we need to create a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LgMdc3xo2AK"
      },
      "source": [
        "# Create training and test set\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE9pu31U4srj"
      },
      "source": [
        "And then we need to check whether out features need to be standardized or normalized. Let's have a look at them. Remember that if features can have both negative and positive values, as a rule of thumb, they should be standardized. If they only have positive values, a normalization is usually used. \n",
        "\n",
        "As already said, please remember that you should learn the standardization/normalization (namely learn the average/std or the max/min values) ONLY in the training set and then use the same values also in the test set. You should NOT use the entire dataset (both training and test) for standardization/normalization. Otherwise, you would have a *data leakage*, namely you would use data (the test set) that you should not use during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI7Z_wPG5Del"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.boxplot(X,notch=True);\n",
        "\n",
        "# Standardization/Normalization\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1HN5Uy2o2AO"
      },
      "source": [
        "Then, we can fit a Decision tree, with the default setting, using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTe1dj0No2AP"
      },
      "source": [
        "# Fitting Decision Trees \n",
        "Tree = DecisionTreeClassifier(random_state=0)\n",
        "Tree.fit(X_train,y_train)\n",
        "# Score in the training set\n",
        "print('Score in the training set is {0}'.format(Tree.score(X_train,y_train)) )\n",
        "# Score in the test set\n",
        "print('Score in the test set is {0}'.format(Tree.score(X_test,y_test)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5HzkZCPo2Ab"
      },
      "source": [
        "Instead than using the default hyperparameters, we could also look for the best ones. Among the hyperparameters implemented in *scikit-learn* we could use *'min_samples_split'*, the minimum number of samples required to split an internal node, and/or *'min_samples_leaf'*, the minimum number of samples required to be present at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
        "\n",
        "Plot the training and test score for different values of 'min_samples_split' (for instance between 2 and 15) WITHOUT using Cross Validation. Do the same for 'min_samples_leaf'.\n",
        "\n",
        "**Question:** What is the best value ? What happens if you split differently your data (change `random_state`in the function `train_test_split`) ? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKVCOi5oo2Ac"
      },
      "source": [
        "# min_samples_split\n",
        "\n",
        "TTest=[]\n",
        "TTrain=[]\n",
        "for i in range(2,16):\n",
        "    Tree = DecisionTreeClassifier(min_samples_split=XXXXXXXXX, random_state=0)\n",
        "    Tree.fit(XXXXXXXXX)\n",
        "    scoreTrain=Tree.score(XXXXXXXXX)\n",
        "    scoreTest=Tree.score(XXXXXXXXX)\n",
        "    TTrain.append(scoreTrain)\n",
        "    TTest.append(scoreTest)\n",
        "plt.plot(TTrain,label='Training error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "print(\"The value of min_samples_split that maximizes the training score is : \",TTrain.index(max(TTrain))+2)\n",
        "plt.plot(TTest,label='Test error');\n",
        "plt.xticks(np.arange(14), ('2', '3', '4', '5', '6','7','8','9','10','11','12','13','14','15'))\n",
        "plt.xlabel('min samples split')\n",
        "plt.ylabel('score')\n",
        "print(\"The value of min_samples_split that maximizes the test score is : \",TTest.index(max(TTest))+2)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3d7pHgoo2Aj"
      },
      "source": [
        "# min_samples_leaf\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI1gFa97o2Ao"
      },
      "source": [
        "Try to add Gaussian noise to the data (using for instance zero mean and 0.05 for $\\sigma$) and, using the best hyperparameters found before in the test set (you can use both `min_samples_leaf` and `min_samples_split`), look at the test score. Repeat this process several times and compare the results with the score obtained without adding noise. \n",
        "\n",
        "**Question**: Are the results stable ? Hint: you could use for instance *noise = np.random.normal(mu, sigma)* if you have standardized the features and *noise = np.abs(np.random.normal(mu, sigma))* if you have normalized them (we use *np.asb()* to take only positive values and $\\sigma$ should be small in order to (almost) preserve the range of the features between 0 and 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rppiIbQFo2Ap"
      },
      "source": [
        "Tree = DecisionTreeClassifier(min_samples_split=XXXXXXXXXXX,min_samples_leaf=XXXXXXXXXXX,random_state=0)\n",
        "scoreTestnoise=np.zeros(100)\n",
        "scoreTest=np.zeros(100)\n",
        "\n",
        "Tree.fit(XXXXXXXXXXX,XXXXXXXXXXX)\n",
        "scoreTest[:] = Tree.score(XXXXXXXXXXX,XXXXXXXXXXX)  \n",
        "\n",
        "for k in range(100):    \n",
        "    X_train_temp=np.copy(X_train)\n",
        "    for i in range(X_train.shape[0]):\n",
        "        X_train_temp[i] XXXXXXXXXXX\n",
        "        \n",
        "    Tree.fit(XXXXXXXXXXX,XXXXXXXXXXX)\n",
        "    scoreTestnoise[k] = Tree.score(XXXXXXXXXXX,XXXXXXXXXXX)\n",
        "    \n",
        "plt.plot(scoreTestnoise,'b',label='noise')\n",
        "plt.plot(scoreTest,'r',linewidth=5.0,label='original')       \n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrLs1nn8o2AU"
      },
      "source": [
        "To plot decision trees, we can also use the *graphviz* library. If you need to install it locally, you can do it using *conda install python-graphviz*. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkOG-YOvks1E"
      },
      "source": [
        "First plot the tree learnt on the original data, witout adding noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2fXiMYIo2AW"
      },
      "source": [
        "import graphviz \n",
        "\n",
        "Tree.fit(X_train,y_train)\n",
        "dot_data = tree.export_graphviz(Tree, out_file=None,feature_names=labels,class_names=class_names,filled=True, rounded=True,special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPDvOh79mcrj"
      },
      "source": [
        "Now, plot the tree learnt on noisy data.\n",
        "\n",
        "**Question**: Is it the same ? You can try several times, for different levels of noise. Comment the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovWKd3PUlwh6"
      },
      "source": [
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLQwNNCco2At"
      },
      "source": [
        "Instead than using a single split of the data, we could also use Cross Validation to compute the best hyperparameter values for both 'min_samples_split' and 'min_samples_leaf' at the same time and in an automatic way. \n",
        "\n",
        "**Question:** Do you find the same optimal hyperparameters as before ? Hint: use GridSearchCV\n",
        "\n",
        "**Question**: So far, we have used the standard score (ie accuracy). Would you use a different one ? If yes, which one and why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDvf_RYTo2Av"
      },
      "source": [
        "Tree = XXXXXXXXXXX\n",
        "p_grid_tree = {XXXXXXXXXXX} \n",
        "grid_tree = GridSearchCV(XXXXXXXXXXX)\n",
        "grid_tree.fit(XXXXXXXXXXX)\n",
        "print(\"Best Validation Score: {}\".format(grid_tree.best_score_))\n",
        "print(\"Best params: {}\".format(grid_tree.best_params_))\n",
        "print(\"Tree test score :\",grid_tree.score(XXXXXXXXXXX))\n",
        "\n",
        "best_params=grid_tree.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC7SNh5-o2A2"
      },
      "source": [
        "Using the estimated optimal hyperparameers, plot the new decision tree using the *graphviz* library. \n",
        "\n",
        "**Question**: Is it the same as before? Do you see ROIs that are always close to the root of the tree among the different experiments ? If yes, what does it mean in your opinion ? Comment the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5ucpzowo2A4"
      },
      "source": [
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK17gOu9o2A9"
      },
      "source": [
        "Try to use now Bagging. You can use the following code where we use the previously computed best parameters 'min_samples_leaf' and 'min_samples_split'. \n",
        "\n",
        "**Question**: What happens when you use the original data and the noisy version ? Do you notice any difference in the prediction scores with respect to the results using Decision Trees ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij465Vaho2A-"
      },
      "source": [
        "Tree = DecisionTreeClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],min_samples_split=best_params[\"min_samples_split\"], random_state=0)\n",
        "\n",
        "p_grid_bagging = {'n_estimators': [5,10,15,20]}      \n",
        "bag=BaggingClassifier(base_estimator=Tree, random_state=0)\n",
        "grid_bagging = GridSearchCV(estimator=bag, XXXXXXXXXXX)\n",
        "grid_bagging.fit(XXXXXXXXXXX, XXXXXXXXXXX)\n",
        "print(\"Best Validation Score: {}\".format(grid_bagging.best_score_))\n",
        "print(\"Best params: {}\".format(grid_bagging.best_params_))\n",
        "print(\"Bagging test score :\",grid_bagging.score(XXXXXXXXXXX,XXXXXXXXXXX))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OSfGQwo2BC"
      },
      "source": [
        "# Bagging on noisy data\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHcxeyfo2BF"
      },
      "source": [
        "The last part of this TP is about Random Forests. We can estimate the three hyperparameters *'n_estimators'*, *'min_samples_leaf'* and *'max_features'*, the number of features to consider when looking for the best split, as before using Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BE-H8ywo2BG"
      },
      "source": [
        "RF=RandomForestClassifier(random_state=0)\n",
        "p_grid_RF = {'n_estimators': [10,15,20,25,30], 'min_samples_leaf': [2,3,4,5,6], 'max_features': ['sqrt','log2']}   \n",
        "\n",
        "grid_RF = GridSearchCV(estimator=XXXXXXXXXXX, param_grid=XXXXXXXXXXX, scoring=XXXXXXXXXXX, cv=5)\n",
        "grid_RF.fit(XXXXXXXXXXX, XXXXXXXXXXX)\n",
        "\n",
        "print(\"Best Validation Score: {}\".format(grid_RF.best_score_))\n",
        "print(\"Best params: {}\".format(grid_RF.best_params_))\n",
        "print(\"Random Forest test score :\",grid_RF.score(XXXXXXXXXXX,XXXXXXXXXXX))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI414cecG3i3"
      },
      "source": [
        "Using the estimated best hyperparameters, test the performance of Random Forest on the noisy data and compare the results with Decision Trees and Bagging. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnwoUhr3G6b1"
      },
      "source": [
        "# Random Forest on noisy data\n",
        "XXXXXXXXXXX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPiUrNAo2BN"
      },
      "source": [
        "We can also use Random Forests to check the importance of the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7PU3C8ko2BQ"
      },
      "source": [
        "best_params=grid_RF.best_params_\n",
        "RF = RandomForestClassifier(min_samples_leaf=best_params[\"min_samples_leaf\"],max_features=best_params[\"max_features\"],n_estimators=best_params[\"n_estimators\"], random_state=0)\n",
        "RF.fit(X_train,y_train)\n",
        "\n",
        "importances = RF.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(10):\n",
        "    print(\"%d. feature %d representing %s (%f)\" % (f + 1, indices[f], labels[indices[f]], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importance\")\n",
        "plt.bar(range(10), importances[indices[0:10]], color=\"r\", align=\"center\")\n",
        "plt.xticks(range(10), indices[0:10])\n",
        "plt.xlim([-1, 10])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiNUODxOo2BU"
      },
      "source": [
        "**Question**: Which are the most important features (i.e. ROIs) ?  Based on the two given research papers, you can verify if your results make sense. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psuqzjU5o2BY"
      },
      "source": [
        "We can also inspect the data using only pairs of the most important features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om2OxZ3bo2Bb"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "for pairidx, pair in enumerate([ [indices[0],indices[1]], [indices[0],indices[2]], [indices[0],indices[3]],\n",
        "                                [indices[1],indices[2]], [indices[1],indices[3]], [indices[2],indices[3]] ]):\n",
        "    # We only take the two corresponding features\n",
        "    Xpair = X_train[:, pair]\n",
        "    ypair = y_train\n",
        "\n",
        "    # Train\n",
        "    clf = RF.fit(Xpair, ypair)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = Xpair[:, 0].min() - 1, Xpair[:, 0].max() + 1\n",
        "    y_min, y_max = Xpair[:, 1].min() - 1, Xpair[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    plt.xlabel(pair[0])\n",
        "    plt.ylabel(pair[1])\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(2), \"ym\"):\n",
        "        idx = np.where(ypair == i)\n",
        "        plt.scatter(Xpair[idx, 0], Xpair[idx, 1], c=color, label=class_names[i],\n",
        "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
        "\n",
        "plt.suptitle(\"Decision surface of a random forest using couples of the most important features\")\n",
        "plt.legend(bbox_to_anchor=(1, 0.5))\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJMWJrFh0mGO"
      },
      "source": [
        "**Question**: Which is the best couple of features ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZjHYuz7sAIU"
      },
      "source": [
        "**Different Atlas**\n",
        "\n",
        "Previously, we have used the AAL2 which defines a precise split of the brain into ROIs. What happens if you change Atlas ? Do you obtain the same results ? Can you find a subset of ROIs that you could define 'biomarkers' of the Alzheimer's disease ? Justify your answer and check whether it makes sense by using the two given research papers.\n",
        "\n",
        "You can use the AICHA (http://www.gin.cnrs.fr/fr/outils/aicha/) and HAMMERS (https://brain-development.org/brain-atlases/adult-brain-atlases/) atlas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Nxl8vNObJQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}